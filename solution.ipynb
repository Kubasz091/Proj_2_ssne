{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing danych"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "money = 100000\n",
    "debt = 250000\n",
    "\n",
    "print(\"cheap: \", 0, \" - \",money)\n",
    "print(\"average: \", money, \" - \", debt+money)\n",
    "print(\"expensive: \", debt+money, \" - \", \"inf\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initial data viewing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('zadanie_studenci/train_data.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "test_df = pd.read_csv('zadanie_studenci/test_data.csv')\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "columns_with_types = {}\n",
    "\n",
    "for column in df.columns:\n",
    "    numeric_series = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    if not numeric_series.isna().any():\n",
    "        if (numeric_series == numeric_series.astype(int)).all():\n",
    "            print(f\"{column} has type: int\")\n",
    "            columns_with_types[column] = \"int\"\n",
    "        else:\n",
    "            print(f\"{column} has type: float\")\n",
    "            columns_with_types[column] = \"float\"\n",
    "    else:\n",
    "        print(f\"{column} has type: string\")\n",
    "        columns_with_types[column] = \"string\"\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "columns_with_types"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numerical_columns = []\n",
    "categorical_columns = []\n",
    "\n",
    "for column, _type in columns_with_types.items():\n",
    "    if _type == \"int\" or _type == \"float\":\n",
    "        numerical_columns.append(column)\n",
    "    else:\n",
    "        categorical_columns.append(column)\n",
    "\n",
    "print(\"Numerical columns: \", numerical_columns)\n",
    "print(\"Categorical columns: \", categorical_columns)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[numerical_columns] = df[numerical_columns].apply(pd.to_numeric, errors='coerce')"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for colm in categorical_columns:\n",
    "    value_counts = df[colm].value_counts()\n",
    "    print(f\"{colm} has unique values: {value_counts.to_dict()}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['SalePrice'] = df['SalePrice'].astype(int)\n",
    "\n",
    "min_target = df['SalePrice'].min()\n",
    "max_target = df['SalePrice'].max()\n",
    "\n",
    "def categorize_target(value):\n",
    "    if value < money:\n",
    "        return 0\n",
    "    elif value < debt + money:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df['price_category'] = df['SalePrice'].apply(categorize_target)\n",
    "df.drop(columns=['SalePrice'], inplace=True)\n",
    "\n",
    "target_counts = df['price_category'].value_counts().to_dict()\n",
    "\n",
    "print(\"Target counts: \", target_counts)\n",
    "print(\"Max target: \", max_target)\n",
    "print(\"Min target: \", min_target)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# With this knowledge we start data preprocessing\n",
    "\n",
    "- Plan is like this: for now to leave the numerical vals as they are and to just convert the data to ints\n",
    "- for coulumns TimeToBusStop, TimeToSubway do index encoding, as is thinkt that actually it will be good for the model to have a correlation that 10-15 minutes is closer to 5-10mins then 0-5 mins\n",
    "- turn year built to years old"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Label encoding"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "idx_bus_stop = data[0].index(\"TimeToBusStop\")\n",
    "idx_subway = data[0].index(\"TimeToSubway\")\n",
    "idx_heating = data[0].index(\"HeatingType\")\n",
    "idx_apt_manage = data[0].index(\"AptManageType\")\n",
    "\n",
    "bus_stop_mapping_str_to_int = {\"0~5min\": 3, \"5min~10min\": 2, \"10min~15min\": 1}\n",
    "bus_stop_mapping_int_to_str = {v: k for k, v in bus_stop_mapping_str_to_int.items()}\n",
    "\n",
    "subway_mapping_str_to_int = {\"no_bus_stop_nearby\": 0, \"0-5min\": 4, \"5min~10min\": 3, \"10min~15min\": 2, \"15min~20min\": 1}\n",
    "subway_mapping_int_to_str = {v: k for k, v in subway_mapping_str_to_int.items()}\n",
    "\n",
    "heat_mapping_str_to_int = {\"individual_heating\": 1, \"central_heating\": 0}\n",
    "heat_mapping_int_to_str = {v: k for k, v in heat_mapping_str_to_int.items()}\n",
    "\n",
    "apt_mapping_str_to_int = {\"management_in_trust\": 1, \"self_management\": 0}\n",
    "apt_mapping_int_to_str = {v: k for k, v in apt_mapping_str_to_int.items()}\n",
    "\n",
    "df['TimeToBusStop'] = df['TimeToBusStop'].map(bus_stop_mapping_str_to_int)\n",
    "df['TimeToSubway'] = df['TimeToSubway'].map(subway_mapping_str_to_int)\n",
    "df['HeatingType'] = df['HeatingType'].map(heat_mapping_str_to_int)\n",
    "df['AptManageType'] = df['AptManageType'].map(apt_mapping_str_to_int)\n",
    "\n",
    "print(df[['TimeToBusStop', 'TimeToSubway', 'HeatingType', 'AptManageType']].head())\n",
    "\n",
    "print(\"Unique values in TimeToBusStop:\", df['TimeToBusStop'].unique())\n",
    "print(\"Unique values in TimeToSubway:\", df['TimeToSubway'].unique())\n",
    "print(\"Unique values in HeatingType:\", df['HeatingType'].unique())\n",
    "print(\"Unique values in AptManageType:\", df['AptManageType'].unique())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe(include='all')"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['YearBuilt'] = 2015 - df['YearBuilt']"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe(include='all')"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.get_dummies(df, columns=['HallwayType', 'SubwayStation'])\n",
    "\n",
    "bool_columns = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "df[bool_columns] = df[bool_columns].astype(int)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe(include='all')"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "numerical_columns.remove('SalePrice')"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target = df['price_category']\n",
    "data = df.drop(columns=['price_category'])\n",
    "\n",
    "data_num = data[numerical_columns]\n",
    "data_cat = data[data.columns.difference(numerical_columns)]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bootstrapping"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def data_weights(dataframe):\n",
    "    weights = np.ones(len(dataframe))\n",
    "    for col in dataframe.columns:\n",
    "        val_counts = dataframe[col].value_counts(normalize=True)\n",
    "        if col not in ['TimeToBusStop', 'TimeToSubway', 'HeatingType', 'AptManageType']:\n",
    "            for value, count in val_counts.items():\n",
    "                weights[dataframe[col] == value] *= (1 - count / len(dataframe))\n",
    "        else:\n",
    "            weights[dataframe[col] == 1] *= (1 - (dataframe[col] == 1).sum() / len(dataframe))\n",
    "    weights_series = pd.Series(weights, index=dataframe.index)\n",
    "    weights_series = weights_series * (1 / weights_series.mean())\n",
    "    return weights_series\n",
    "data_weights = data_weights(data_cat)\n",
    "print(data_weights.mean())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# indices = np.arange(len(data))\n",
    "# t_indices = np.arange(0.8 * len(data)) # 80% for training\n",
    "# train_indices = np.random.choice(\n",
    "#     t_indices,\n",
    "#     size=10000,\n",
    "#     replace=True,\n",
    "#     p=data_weights.iloc[t_indices].values / data_weights.iloc[t_indices].values.sum())\n",
    "# val_indices = np.array([i for i in indices if i not in t_indices])\n",
    "#\n",
    "# train_data_num = data_num.iloc[train_indices]\n",
    "# train_data_cat = data_cat.iloc[train_indices]\n",
    "# train_target = target.iloc[train_indices]\n",
    "#\n",
    "# val_data_num = data_num.iloc[val_indices]\n",
    "# val_data_cat = data_cat.iloc[val_indices]\n",
    "# val_target = target.iloc[val_indices]\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_indices = np.random.rand(len(data))>0.1\n",
    "\n",
    "data_num = pd.concat([data_num, data_cat[['TimeToBusStop', 'TimeToSubway', 'HeatingType', 'AptManageType']]], axis=1)\n",
    "data_cat = data_cat.copy()\n",
    "data_cat.drop(columns=['TimeToBusStop', 'TimeToSubway', 'HeatingType', 'AptManageType'], inplace=True)\n",
    "\n",
    "train_data_num = data_num[train_indices]\n",
    "train_data_cat = data_cat[train_indices]\n",
    "train_target = target[train_indices]\n",
    "\n",
    "val_data_num = data_num[~train_indices]\n",
    "val_data_cat = data_cat[~train_indices]\n",
    "val_target = target[~train_indices]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# normalize the data\n",
    "- test_set the same as train set so that there is no data leakage"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data_num = scaler.fit_transform(train_data_num)\n",
    "val_data_num = scaler.transform(val_data_num)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data_num = torch.tensor(train_data_num, dtype=torch.float32)\n",
    "train_data_cat = torch.tensor(train_data_cat.values, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target.values, dtype=torch.int64)\n",
    "\n",
    "val_data_num = torch.tensor(val_data_num, dtype=torch.float32)\n",
    "val_data_cat = torch.tensor(val_data_cat.values, dtype=torch.float32)\n",
    "val_target = torch.tensor(val_target.values, dtype=torch.int64)\n",
    "\n",
    "sample_weights = torch.tensor(data_weights.iloc[train_indices].values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_num, train_data_cat, train_target)\n",
    "test_dataset = torch.utils.data.TensorDataset(val_data_num, val_data_cat, val_target)\n",
    "\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(train_indices),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FlatClassifier(nn.Module):\n",
    "    def __init__(self,size_1,size_2, activation, dropout):\n",
    "        super(FlatClassifier, self).__init__()\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.Linear(train_data_cat.shape[1], train_data_cat.shape[1]*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(train_data_cat.shape[1]*2, train_data_cat.shape[1])\n",
    "        )\n",
    "        self.layer1 = nn.Linear(train_data_num.shape[1] + train_data_cat.shape[1], size_1)\n",
    "        self.bn1 = nn.BatchNorm1d(size_1)\n",
    "        self.act_1 = activation()\n",
    "        self.d1 = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(size_1, size_2)\n",
    "        self.bn2 = nn.BatchNorm1d(size_2)\n",
    "        self.act_2 = activation()\n",
    "        self.d2 = nn.Dropout(dropout)\n",
    "        self.layer3 = nn.Linear(size_2, size_2//2)\n",
    "        self.bn3 = nn.BatchNorm1d(size_2//2)\n",
    "        self.act_3 = activation()\n",
    "        self.output = nn.Linear(size_2//2, 3)\n",
    "\n",
    "    def forward(self, x, cat_x):\n",
    "        cat_x_embedded = self.emb_layer(cat_x)\n",
    "        x = torch.cat([x,cat_x_embedded],dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_1(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act_2(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.act_3(x)\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for x, cat_x, labels in data_loader:\n",
    "        x, cat_x, labels = x.to(device), cat_x.to(device), labels.to(device)\n",
    "        output = model(x, cat_x)\n",
    "        pred = output>0\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += x.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = FlatClassifier(64, 32, nn.LeakyReLU, 0).to(device)\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0007)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "iters = []\n",
    "losses = []\n",
    "val_losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "best_val_acc = 0\n",
    "patience = 100\n",
    "counter = 0\n",
    "for n in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for num_data, cat_data, target_batch in train_loader:\n",
    "        num_data = num_data.float().to(device)\n",
    "        cat_data = cat_data.float().to(device)\n",
    "        target_batch = target_batch.long().to(device)\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(num_data, cat_data)\n",
    "        loss = criterion(outputs, target_batch)\n",
    "\n",
    "        weighted_loss = loss.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(weighted_loss.item())\n",
    "\n",
    "    loss_mean = np.array(epoch_losses).mean()\n",
    "    iters.append(n)\n",
    "    losses.append(loss_mean)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_epoch_losses = []\n",
    "        for num_data, cat_data, target_batch in test_loader:\n",
    "            num_data = num_data.float().to(device)\n",
    "            cat_data = cat_data.float().to(device)\n",
    "            target_batch = target_batch.long().to(device)\n",
    "            outputs = model(num_data, cat_data)\n",
    "            valid_loss = criterion(outputs, target_batch).mean().item()\n",
    "            valid_epoch_losses.append(valid_loss)\n",
    "\n",
    "        val_loss_mean = np.array(valid_epoch_losses).mean()\n",
    "        val_losses.append(val_loss_mean)\n",
    "\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for num_data, cat_data, target_batch in train_loader:\n",
    "            num_data = num_data.float().to(device)\n",
    "            cat_data = cat_data.float().to(device)\n",
    "            target_batch = target_batch.long().to(device)\n",
    "            outputs = model(num_data, cat_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += target_batch.size(0)\n",
    "            correct_train += (predicted == target_batch).sum().item()\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        for num_data, cat_data, target_batch in test_loader:\n",
    "            num_data = num_data.float().to(device)\n",
    "            cat_data = cat_data.float().to(device)\n",
    "            target_batch = target_batch.long().to(device)\n",
    "            outputs = model(num_data, cat_data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += target_batch.size(0)\n",
    "            correct_test += (predicted == target_batch).sum().item()\n",
    "        test_accuracy = correct_test / total_test\n",
    "\n",
    "        if test_accuracy > best_val_acc:\n",
    "            best_val_acc = test_accuracy\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {n}\")\n",
    "                break\n",
    "\n",
    "    train_acc.append(train_accuracy)\n",
    "    val_acc.append(test_accuracy)\n",
    "    scheduler.step(loss_mean)\n",
    "\n",
    "    print(f\"Epoch {n} train_loss {loss_mean:.3f} val_loss {val_loss_mean:.3f} train_acc: {train_accuracy:.3f} test_acc: {test_accuracy:.3f}\")\n",
    "\n",
    "print(f\"Loading best model from checkpoint...\")\n",
    "model = FlatClassifier(64, 32, nn.LeakyReLU, 0).to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for num_data, cat_data, target_batch in train_loader:\n",
    "        num_data = num_data.float().to(device)\n",
    "        cat_data = cat_data.float().to(device)\n",
    "        target_batch = target_batch.long().to(device)\n",
    "        outputs = model(num_data, cat_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += target_batch.size(0)\n",
    "        correct_train += (predicted == target_batch).sum().item()\n",
    "    best_train_accuracy = correct_train / total_train\n",
    "\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    for num_data, cat_data, target_batch in test_loader:\n",
    "        num_data = num_data.float().to(device)\n",
    "        cat_data = cat_data.float().to(device)\n",
    "        target_batch = target_batch.long().to(device)\n",
    "        outputs = model(num_data, cat_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += target_batch.size(0)\n",
    "        correct_test += (predicted == target_batch).sum().item()\n",
    "    best_test_accuracy = correct_test / total_test\n",
    "\n",
    "print(\"Best Model Training Accuracy: {:.3f}\".format(best_train_accuracy))\n",
    "print(\"Best Model Validation Accuracy: {:.3f}\".format(best_test_accuracy))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_len = min(len(iters), len(losses), len(val_losses), len(train_acc), len(val_acc))\n",
    "print(f\"Number of epochs tracked: {max_len}\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.plot(iters[:max_len], losses[:max_len], label=\"Train\")\n",
    "plt.plot(iters[:max_len], val_losses[:max_len], label=\"Validation\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Training Curve\")\n",
    "plt.plot(iters[:max_len], train_acc[:max_len], label=\"Train\")\n",
    "plt.plot(iters[:max_len], val_acc[:max_len], label=\"Validation\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ladujemy zbior testowy i generujemy predykcje do pliku"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('zadanie_studenci/test_data.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "df_t = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "missing_values = df_t.isnull().sum()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# do the same data preprocessing as in training dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_t[numerical_columns] = df_t[numerical_columns].apply(pd.to_numeric, errors='coerce')"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_t['TimeToBusStop'] = df_t['TimeToBusStop'].map(bus_stop_mapping_str_to_int)\n",
    "df_t['TimeToSubway'] = df_t['TimeToSubway'].map(subway_mapping_str_to_int)\n",
    "df_t['HeatingType'] = df_t['HeatingType'].map(heat_mapping_str_to_int)\n",
    "df_t['AptManageType'] = df_t['AptManageType'].map(apt_mapping_str_to_int)\n",
    "\n",
    "df_t['YearBuilt'] = 2015 - df_t['YearBuilt']\n",
    "\n",
    "df_t = pd.get_dummies(df_t, columns=['HallwayType', 'SubwayStation'])\n",
    "\n",
    "bool_columns = df_t.select_dtypes(include=['bool']).columns.tolist()\n",
    "df_t[bool_columns] = df_t[bool_columns].astype(int)\n",
    "\n",
    "test_data_num = pd.concat([\n",
    "    df_t[numerical_columns],\n",
    "    df_t[['TimeToBusStop', 'TimeToSubway', 'HeatingType', 'AptManageType']]\n",
    "], axis=1)\n",
    "\n",
    "test_data_scaled = scaler.transform(test_data_num)\n",
    "\n",
    "df_t[test_data_num.columns] = test_data_scaled"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_t.describe(include='all')"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get numerical data\n",
    "val_data_num = df_t[numerical_columns]\n",
    "\n",
    "# Add the categorical features that should be treated as numerical\n",
    "cat_features_to_move = ['TimeToBusStop', 'TimeToSubway', 'HeatingType', 'AptManageType']\n",
    "val_data_num = pd.concat([val_data_num, df_t[cat_features_to_move]], axis=1)\n",
    "\n",
    "# Get remaining categorical data (excluding the ones moved to numerical)\n",
    "val_data_cat = df_t[df_t.columns.difference(numerical_columns)]\n",
    "val_data_cat = val_data_cat.drop(columns=cat_features_to_move)\n",
    "\n",
    "# Convert to tensors\n",
    "val_data_num = torch.tensor(val_data_num.values, dtype=torch.float32)\n",
    "val_data_cat = torch.tensor(val_data_cat.values, dtype=torch.float32)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(val_data_num, val_data_cat)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for num_data, cat_data in test_loader:\n",
    "        num_data = num_data.float().to(device)\n",
    "        cat_data = cat_data.float().to(device)\n",
    "        outputs = model(num_data, cat_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(predictions[:10])"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# save the predictions as a csv file"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('zadanie_studenci/predictions.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for pred in predictions:\n",
    "        writer.writerow([pred])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# co jeszcze dodac?\n",
    "- koniec notebooka lab4 - wagi do przykładów\n",
    "- inny preprocessing danych jak sie bedzie chciało\n",
    "- model nie przekopiowany z notebooka z labów\n",
    "- eksperymentacja z optymizerem\n",
    "- embedding jak sie bedzie chciało\n",
    "- walka z przeuczeniem, bachnorm, regularization, weight decay itd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
